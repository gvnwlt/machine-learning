# Naive Bayes Theorem

                P(B|A) * P(A)
P(A|B) = -----------------------------       // probability of A given ('|') B's features
                     P(B)

  *P(A|B) --> posterior probability
  *P(B|A) --> likelihood 
  *P(A) --> prior probability 
  *P(B) --> marginal likelihood 

# Naive Bayes Implement 
1) Find probability for out A given X features
  a) Prior Probability (occurance of outcome / total number of obs)
  b) Marginal Likelihood (using input feature, draw radius around it to include other observations that are similar; use the observations found in radius / total)
  c) Likelihood (use same radius in marginal and determine probabilty of B given A so obs in circle  / total A obs )
  d) Posterior Probability (calculate by using solutions found in the previous steps)
2) Find probability for B given X features
  *repeat step 1 but using B given A now 
3) Compare Probability of A and B results from steps 1 and 2. The larger probability means it is more likely for one to happen over the other (P > A). 

# Naive Bayes Use Case
  Bayes Theorem requires some independence assumptions which are often times not correct, thus being naive. 

  It's applied to features that are not completely independent. But it still gives good results.

# Multiple Classes 
  